"""
Evaluator for PromptCraft.
"""
import json
import os


class Evaluator:
    """Handles evaluation of candidate submissions."""
    
    def __init__(self, output_dir="evaluation_results"):
        """Initialize the evaluator with the output directory."""
        self.output_dir = output_dir
        # Create output directory if it doesn't exist
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
    def evaluate_submission(self, candidate_id, task_id, prompt, 
                          generated_code=None, evaluation_criteria=None):
        """
        Evaluate a candidate's submission.
        
        Args:
            candidate_id: Identifier for the candidate
            task_id: Identifier for the task
            prompt: The candidate's prompt text
            generated_code: The code generated by LLM (if available)
            evaluation_criteria: List of criteria to evaluate against
            
        Returns:
            Dictionary with evaluation results
        """
        print(f"\n--- Evaluation for Candidate {candidate_id}, Task {task_id} ---")
        print(f"Candidate's Prompt: {prompt}")
        
        if generated_code:
            print(f"Generated Code:\n{generated_code}")
            
        print("Evaluation notes:")
        if evaluation_criteria:
            print("Based on the following criteria:")
            for criterion in evaluation_criteria:
                print(f"- {criterion}")
                
        # In a full implementation, you might have interactive scoring here,
        # or integration with automated evaluation tools
        evaluation_notes = input("> ")
        
        # Create a structured evaluation result
        evaluation_result = {
            "candidate_id": candidate_id,
            "task_id": task_id,
            "evaluation_notes": evaluation_notes,
            "scores": {}  # Could be populated with structured scoring
        }
        
        # Save evaluation to file
        self._save_evaluation(evaluation_result)
        
        return evaluation_result
        
    def _save_evaluation(self, evaluation_result):
        """Save evaluation results to a file."""
        candidate_id = evaluation_result["candidate_id"]
        task_id = evaluation_result["task_id"]
        
        filename = f"{self.output_dir}/eval_{candidate_id}_task{task_id}.json"
        with open(filename, 'w') as f:
            json.dump(evaluation_result, f, indent=2)
            
        print(f"Evaluation saved to '{filename}'")
        
    def get_candidate_evaluations(self, candidate_id):
        """Retrieve all evaluations for a specific candidate."""
        evaluations = []
        
        # Look for all evaluation files for this candidate
        prefix = f"eval_{candidate_id}_"
        for filename in os.listdir(self.output_dir):
            if filename.startswith(prefix) and filename.endswith(".json"):
                with open(os.path.join(self.output_dir, filename), 'r') as f:
                    evaluation = json.load(f)
                    evaluations.append(evaluation)
                    
        return evaluations 