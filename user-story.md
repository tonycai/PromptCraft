# User Stories

As a **Technical Interviewer/Hiring Manager**,
I want **a tool that allows me to effectively assess a candidate's ability to guide Large Language Models (LLMs) for code generation through well-crafted prompts**,
so that **I can identify candidates who can leverage AI effectively in software development workflows.**

---

As a **Technical Interviewer/Hiring Manager**,
I want **to define a variety of code generation tasks with clear descriptions and expected outcomes**,
so that **I can evaluate candidates across different programming languages, complexities, and problem domains.**

---

As a **Technical Interviewer/Hiring Manager**,
I want **to present these tasks to candidates in a structured manner and record their submitted prompts**,
so that **I have a clear and auditable record of their approach and communication with the simulated (or real) LLM.**

---

As a **Technical Interviewer/Hiring Manager**,
I want **the tool to optionally simulate the response of an LLM based on the candidate's prompt (for initial testing or in environments without direct LLM access)**,
so that **I can get a preliminary understanding of the prompt's effectiveness without relying solely on manual LLM interaction.**

---

As a **Technical Interviewer/Hiring Manager**,
I want **to easily review the candidate's prompts and the (simulated or actual) generated code within a structured output**,
so that **I can efficiently evaluate their prompting skills, understanding of programming concepts, and ability to debug/refine the LLM's output.**

---

As a **Project Maintainer**,
I want **a well-documented and extensible Python framework**,
so that **I can easily add new tasks, integrate with different LLM APIs, and enhance the evaluation capabilities of the tool in the future.**

---

As a **Candidate (in a potential future story for self-assessment)**,
I want **clear instructions and well-defined tasks**,
so that **I can understand the expectations and demonstrate my ability to effectively prompt LLMs for code generation.**
